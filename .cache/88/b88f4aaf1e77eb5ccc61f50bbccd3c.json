{"id":"../node_modules/openai/resources/audio/transcriptions.js","dependencies":[{"name":"D:\\Licenta\\node_modules\\openai\\resources\\audio\\transcriptions.js.map","includedInParent":true,"mtime":1715683698926},{"name":"D:\\Licenta\\node_modules\\openai\\src\\resources\\audio\\transcriptions.ts","includedInParent":true,"mtime":1715683699008},{"name":"D:\\Licenta\\package.json","includedInParent":true,"mtime":1715683699049},{"name":"D:\\Licenta\\node_modules\\openai\\package.json","includedInParent":true,"mtime":1715683698824},{"name":"../../resource.js","loc":{"line":5,"column":27,"index":224},"parent":"D:\\Licenta\\node_modules\\openai\\resources\\audio\\transcriptions.js","resolved":"D:\\Licenta\\node_modules\\openai\\resource.js"},{"name":"../../core.js","loc":{"line":6,"column":23,"index":269},"parent":"D:\\Licenta\\node_modules\\openai\\resources\\audio\\transcriptions.js","resolved":"D:\\Licenta\\node_modules\\openai\\core.js"}],"generated":{"js":"\"use strict\";\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.Transcriptions = void 0;\nconst resource_1 = require(\"../../resource.js\");\nconst core_1 = require(\"../../core.js\");\nclass Transcriptions extends resource_1.APIResource {\n    /**\n     * Transcribes audio into the input language.\n     */\n    create(body, options) {\n        return this._client.post('/audio/transcriptions', (0, core_1.multipartFormRequestOptions)({ body, ...options }));\n    }\n}\nexports.Transcriptions = Transcriptions;\n(function (Transcriptions) {\n})(Transcriptions = exports.Transcriptions || (exports.Transcriptions = {}));\n"},"sourceMaps":{"js":{"version":3,"file":"transcriptions.js","sourceRoot":"","sources":["../../src/resources/audio/transcriptions.ts"],"names":[],"mappings":";AAAA,sFAAsF;;;AAGtF,gDAA6C;AAE7C,wCAA0E;AAE1E,MAAa,cAAe,SAAQ,sBAAW;IAC7C;;OAEG;IACH,MAAM,CAAC,IAA+B,EAAE,OAA6B;QACnE,OAAO,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,uBAAuB,EAAE,IAAA,kCAA2B,EAAC,EAAE,IAAI,EAAE,GAAG,OAAO,EAAE,CAAC,CAAC,CAAC;IACvG,CAAC;CACF;AAPD,wCAOC;AAkED,WAAiB,cAAc;AAG/B,CAAC,EAHgB,cAAc,GAAd,sBAAc,KAAd,sBAAc,QAG9B","sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport * as Core from '../../core';\nimport { APIResource } from '../../resource';\nimport * as TranscriptionsAPI from './transcriptions';\nimport { type Uploadable, multipartFormRequestOptions } from '../../core';\n\nexport class Transcriptions extends APIResource {\n  /**\n   * Transcribes audio into the input language.\n   */\n  create(body: TranscriptionCreateParams, options?: Core.RequestOptions): Core.APIPromise<Transcription> {\n    return this._client.post('/audio/transcriptions', multipartFormRequestOptions({ body, ...options }));\n  }\n}\n\n/**\n * Represents a transcription response returned by model, based on the provided\n * input.\n */\nexport interface Transcription {\n  /**\n   * The transcribed text.\n   */\n  text: string;\n}\n\nexport interface TranscriptionCreateParams {\n  /**\n   * The audio file object (not file name) to transcribe, in one of these formats:\n   * flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` (which is powered by our open source\n   * Whisper V2 model) is currently available.\n   */\n  model: (string & {}) | 'whisper-1';\n\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will\n   * improve accuracy and latency.\n   */\n  language?: string;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The\n   * [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting)\n   * should match the audio language.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the transcript output, in one of these options: `json`, `text`,\n   * `srt`, `verbose_json`, or `vtt`.\n   */\n  response_format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n\n  /**\n   * The timestamp granularities to populate for this transcription.\n   * `response_format` must be set `verbose_json` to use timestamp granularities.\n   * Either or both of these options are supported: `word`, or `segment`. Note: There\n   * is no additional latency for segment timestamps, but generating word timestamps\n   * incurs additional latency.\n   */\n  timestamp_granularities?: Array<'word' | 'segment'>;\n}\n\nexport namespace Transcriptions {\n  export import Transcription = TranscriptionsAPI.Transcription;\n  export import TranscriptionCreateParams = TranscriptionsAPI.TranscriptionCreateParams;\n}\n"]}},"error":null,"hash":"01d470be60177507d1855f788409d071","cacheData":{"env":{}}}