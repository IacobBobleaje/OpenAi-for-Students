{"id":"../node_modules/openai/resources/beta/vector-stores/file-batches.js","dependencies":[{"name":"D:\\Licenta\\node_modules\\openai\\resources\\beta\\vector-stores\\file-batches.js.map","includedInParent":true,"mtime":1715683698971},{"name":"D:\\Licenta\\node_modules\\openai\\src\\resources\\beta\\vector-stores\\file-batches.ts","includedInParent":true,"mtime":1715683699030},{"name":"D:\\Licenta\\package.json","includedInParent":true,"mtime":1715683699049},{"name":"D:\\Licenta\\node_modules\\openai\\package.json","includedInParent":true,"mtime":1715683698824},{"name":"../../../resource.js","loc":{"line":5,"column":27,"index":252},"parent":"D:\\Licenta\\node_modules\\openai\\resources\\beta\\vector-stores\\file-batches.js","resolved":"D:\\Licenta\\node_modules\\openai\\resource.js"},{"name":"../../../core.js","loc":{"line":7,"column":23,"index":344},"parent":"D:\\Licenta\\node_modules\\openai\\resources\\beta\\vector-stores\\file-batches.js","resolved":"D:\\Licenta\\node_modules\\openai\\core.js"},{"name":"../../../lib/Util.js","loc":{"line":8,"column":23,"index":388},"parent":"D:\\Licenta\\node_modules\\openai\\resources\\beta\\vector-stores\\file-batches.js","resolved":"D:\\Licenta\\node_modules\\openai\\lib\\Util.js"},{"name":"./files.js","loc":{"line":9,"column":24,"index":437},"parent":"D:\\Licenta\\node_modules\\openai\\resources\\beta\\vector-stores\\file-batches.js","resolved":"D:\\Licenta\\node_modules\\openai\\resources\\beta\\vector-stores\\files.js"}],"generated":{"js":"\"use strict\";\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.VectorStoreFilesPage = exports.FileBatches = void 0;\nconst resource_1 = require(\"../../../resource.js\");\nconst core_1 = require(\"../../../core.js\");\nconst core_2 = require(\"../../../core.js\");\nconst Util_1 = require(\"../../../lib/Util.js\");\nconst files_1 = require(\"./files.js\");\nObject.defineProperty(exports, \"VectorStoreFilesPage\", { enumerable: true, get: function () { return files_1.VectorStoreFilesPage; } });\nclass FileBatches extends resource_1.APIResource {\n    /**\n     * Create a vector store file batch.\n     */\n    create(vectorStoreId, body, options) {\n        return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieves a vector store file batch.\n     */\n    retrieve(vectorStoreId, batchId, options) {\n        return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Cancel a vector store file batch. This attempts to cancel the processing of\n     * files in this batch as soon as possible.\n     */\n    cancel(vectorStoreId, batchId, options) {\n        return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Create a vector store batch and poll until all files have been processed.\n     */\n    async createAndPoll(vectorStoreId, body, options) {\n        const batch = await this.create(vectorStoreId, body);\n        return await this.poll(vectorStoreId, batch.id, options);\n    }\n    listFiles(vectorStoreId, batchId, query = {}, options) {\n        if ((0, core_1.isRequestOptions)(query)) {\n            return this.listFiles(vectorStoreId, batchId, {}, query);\n        }\n        return this._client.getAPIList(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`, files_1.VectorStoreFilesPage, { query, ...options, headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers } });\n    }\n    /**\n     * Wait for the given file batch to be processed.\n     *\n     * Note: this will return even if one of the files failed to process, you need to\n     * check batch.file_counts.failed_count to handle this case.\n     */\n    async poll(vectorStoreId, batchId, options) {\n        const headers = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\n        if (options?.pollIntervalMs) {\n            headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n        }\n        while (true) {\n            const { data: batch, response } = await this.retrieve(vectorStoreId, batchId, {\n                ...options,\n                headers,\n            }).withResponse();\n            switch (batch.status) {\n                case 'in_progress':\n                    let sleepInterval = 5000;\n                    if (options?.pollIntervalMs) {\n                        sleepInterval = options.pollIntervalMs;\n                    }\n                    else {\n                        const headerInterval = response.headers.get('openai-poll-after-ms');\n                        if (headerInterval) {\n                            const headerIntervalMs = parseInt(headerInterval);\n                            if (!isNaN(headerIntervalMs)) {\n                                sleepInterval = headerIntervalMs;\n                            }\n                        }\n                    }\n                    await (0, core_2.sleep)(sleepInterval);\n                    break;\n                case 'failed':\n                case 'completed':\n                    return batch;\n            }\n        }\n    }\n    /**\n     * Uploads the given files concurrently and then creates a vector store file batch.\n     *\n     * The concurrency limit is configurable using the `maxConcurrency` parameter.\n     */\n    async uploadAndPoll(vectorStoreId, { files, fileIds = [] }, options) {\n        if (files === null || files.length == 0) {\n            throw new Error('No files provided to process.');\n        }\n        const configuredConcurrency = options?.maxConcurrency ?? 5;\n        //We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n        const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n        const client = this._client;\n        const fileIterator = files.values();\n        const allFileIds = [...fileIds];\n        //This code is based on this design. The libraries don't accommodate our environment limits.\n        // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n        async function processFiles(iterator) {\n            for (let item of iterator) {\n                const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\n                allFileIds.push(fileObj.id);\n            }\n        }\n        //Start workers to process results\n        const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n        //Wait for all processing to complete.\n        await (0, Util_1.allSettledWithThrow)(workers);\n        return await this.createAndPoll(vectorStoreId, {\n            file_ids: allFileIds,\n        });\n    }\n}\nexports.FileBatches = FileBatches;\n(function (FileBatches) {\n})(FileBatches = exports.FileBatches || (exports.FileBatches = {}));\n"},"sourceMaps":{"js":{"version":3,"file":"file-batches.js","sourceRoot":"","sources":["../../../src/resources/beta/vector-stores/file-batches.ts"],"names":[],"mappings":";AAAA,sFAAsF;;;AAGtF,mDAAgD;AAChD,2CAAiD;AACjD,2CAAsC;AAEtC,+CAAwD;AAGxD,sCAA+C;AAyRtC,qGAzRA,4BAAoB,OAyRA;AAtR7B,MAAa,WAAY,SAAQ,sBAAW;IAC1C;;OAEG;IACH,MAAM,CACJ,aAAqB,EACrB,IAA2B,EAC3B,OAA6B;QAE7B,OAAO,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,kBAAkB,aAAa,eAAe,EAAE;YACvE,IAAI;YACJ,GAAG,OAAO;YACV,OAAO,EAAE,EAAE,aAAa,EAAE,eAAe,EAAE,GAAG,OAAO,EAAE,OAAO,EAAE;SACjE,CAAC,CAAC;IACL,CAAC;IAED;;OAEG;IACH,QAAQ,CACN,aAAqB,EACrB,OAAe,EACf,OAA6B;QAE7B,OAAO,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,kBAAkB,aAAa,iBAAiB,OAAO,EAAE,EAAE;YACjF,GAAG,OAAO;YACV,OAAO,EAAE,EAAE,aAAa,EAAE,eAAe,EAAE,GAAG,OAAO,EAAE,OAAO,EAAE;SACjE,CAAC,CAAC;IACL,CAAC;IAED;;;OAGG;IACH,MAAM,CACJ,aAAqB,EACrB,OAAe,EACf,OAA6B;QAE7B,OAAO,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,kBAAkB,aAAa,iBAAiB,OAAO,SAAS,EAAE;YACzF,GAAG,OAAO;YACV,OAAO,EAAE,EAAE,aAAa,EAAE,eAAe,EAAE,GAAG,OAAO,EAAE,OAAO,EAAE;SACjE,CAAC,CAAC;IACL,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,aAAa,CACjB,aAAqB,EACrB,IAA2B,EAC3B,OAA2D;QAE3D,MAAM,KAAK,GAAG,MAAM,IAAI,CAAC,MAAM,CAAC,aAAa,EAAE,IAAI,CAAC,CAAC;QACrD,OAAO,MAAM,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE,KAAK,CAAC,EAAE,EAAE,OAAO,CAAC,CAAC;IAC3D,CAAC;IAgBD,SAAS,CACP,aAAqB,EACrB,OAAe,EACf,QAAwD,EAAE,EAC1D,OAA6B;QAE7B,IAAI,IAAA,uBAAgB,EAAC,KAAK,CAAC,EAAE;YAC3B,OAAO,IAAI,CAAC,SAAS,CAAC,aAAa,EAAE,OAAO,EAAE,EAAE,EAAE,KAAK,CAAC,CAAC;SAC1D;QACD,OAAO,IAAI,CAAC,OAAO,CAAC,UAAU,CAC5B,kBAAkB,aAAa,iBAAiB,OAAO,QAAQ,EAC/D,4BAAoB,EACpB,EAAE,KAAK,EAAE,GAAG,OAAO,EAAE,OAAO,EAAE,EAAE,aAAa,EAAE,eAAe,EAAE,GAAG,OAAO,EAAE,OAAO,EAAE,EAAE,CACxF,CAAC;IACJ,CAAC;IAED;;;;;OAKG;IACH,KAAK,CAAC,IAAI,CACR,aAAqB,EACrB,OAAe,EACf,OAA2D;QAE3D,MAAM,OAAO,GAA8B,EAAE,GAAG,OAAO,EAAE,OAAO,EAAE,yBAAyB,EAAE,MAAM,EAAE,CAAC;QACtG,IAAI,OAAO,EAAE,cAAc,EAAE;YAC3B,OAAO,CAAC,kCAAkC,CAAC,GAAG,OAAO,CAAC,cAAc,CAAC,QAAQ,EAAE,CAAC;SACjF;QAED,OAAO,IAAI,EAAE;YACX,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,QAAQ,EAAE,GAAG,MAAM,IAAI,CAAC,QAAQ,CAAC,aAAa,EAAE,OAAO,EAAE;gBAC5E,GAAG,OAAO;gBACV,OAAO;aACR,CAAC,CAAC,YAAY,EAAE,CAAC;YAElB,QAAQ,KAAK,CAAC,MAAM,EAAE;gBACpB,KAAK,aAAa;oBAChB,IAAI,aAAa,GAAG,IAAI,CAAC;oBAEzB,IAAI,OAAO,EAAE,cAAc,EAAE;wBAC3B,aAAa,GAAG,OAAO,CAAC,cAAc,CAAC;qBACxC;yBAAM;wBACL,MAAM,cAAc,GAAG,QAAQ,CAAC,OAAO,CAAC,GAAG,CAAC,sBAAsB,CAAC,CAAC;wBACpE,IAAI,cAAc,EAAE;4BAClB,MAAM,gBAAgB,GAAG,QAAQ,CAAC,cAAc,CAAC,CAAC;4BAClD,IAAI,CAAC,KAAK,CAAC,gBAAgB,CAAC,EAAE;gCAC5B,aAAa,GAAG,gBAAgB,CAAC;6BAClC;yBACF;qBACF;oBACD,MAAM,IAAA,YAAK,EAAC,aAAa,CAAC,CAAC;oBAC3B,MAAM;gBACR,KAAK,QAAQ,CAAC;gBACd,KAAK,WAAW;oBACd,OAAO,KAAK,CAAC;aAChB;SACF;IACH,CAAC;IAED;;;;OAIG;IACH,KAAK,CAAC,aAAa,CACjB,aAAqB,EACrB,EAAE,KAAK,EAAE,OAAO,GAAG,EAAE,EAA+C,EACpE,OAAoF;QAEpF,IAAI,KAAK,KAAK,IAAI,IAAI,KAAK,CAAC,MAAM,IAAI,CAAC,EAAE;YACvC,MAAM,IAAI,KAAK,CAAC,+BAA+B,CAAC,CAAC;SAClD;QAED,MAAM,qBAAqB,GAAG,OAAO,EAAE,cAAc,IAAI,CAAC,CAAC;QAC3D,iGAAiG;QACjG,MAAM,gBAAgB,GAAG,IAAI,CAAC,GAAG,CAAC,qBAAqB,EAAE,KAAK,CAAC,MAAM,CAAC,CAAC;QAEvE,MAAM,MAAM,GAAG,IAAI,CAAC,OAAO,CAAC;QAC5B,MAAM,YAAY,GAAG,KAAK,CAAC,MAAM,EAAE,CAAC;QACpC,MAAM,UAAU,GAAa,CAAC,GAAG,OAAO,CAAC,CAAC;QAE1C,4FAA4F;QAC5F,qHAAqH;QACrH,KAAK,UAAU,YAAY,CAAC,QAAsC;YAChE,KAAK,IAAI,IAAI,IAAI,QAAQ,EAAE;gBACzB,MAAM,OAAO,GAAG,MAAM,MAAM,CAAC,KAAK,CAAC,MAAM,CAAC,EAAE,IAAI,EAAE,IAAI,EAAE,OAAO,EAAE,YAAY,EAAE,EAAE,OAAO,CAAC,CAAC;gBAC1F,UAAU,CAAC,IAAI,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC;aAC7B;QACH,CAAC;QAED,kCAAkC;QAClC,MAAM,OAAO,GAAG,KAAK,CAAC,gBAAgB,CAAC,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC,GAAG,CAAC,YAAY,CAAC,CAAC;QAE7E,sCAAsC;QACtC,MAAM,IAAA,0BAAmB,EAAC,OAAO,CAAC,CAAC;QAEnC,OAAO,MAAM,IAAI,CAAC,aAAa,CAAC,aAAa,EAAE;YAC7C,QAAQ,EAAE,UAAU;SACrB,CAAC,CAAC;IACL,CAAC;CACF;AA9KD,kCA8KC;AAkGD,WAAiB,WAAW;AAI5B,CAAC,EAJgB,WAAW,GAAX,mBAAW,KAAX,mBAAW,QAI3B","sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport * as Core from '../../../core';\nimport { APIResource } from '../../../resource';\nimport { isRequestOptions } from '../../../core';\nimport { sleep } from '../../../core';\nimport { Uploadable } from '../../../core';\nimport { allSettledWithThrow } from '../../../lib/Util';\nimport * as FileBatchesAPI from './file-batches';\nimport * as FilesAPI from './files';\nimport { VectorStoreFilesPage } from './files';\nimport { type CursorPageParams } from '../../../pagination';\n\nexport class FileBatches extends APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(\n    vectorStoreId: string,\n    body: FileBatchCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<VectorStoreFileBatch> {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<VectorStoreFileBatch> {\n    return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n    });\n  }\n\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<VectorStoreFileBatch> {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n    });\n  }\n\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(\n    vectorStoreId: string,\n    body: FileBatchCreateParams,\n    options?: Core.RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n\n  /**\n   * Returns a list of vector store files in a batch.\n   */\n  listFiles(\n    vectorStoreId: string,\n    batchId: string,\n    query?: FileBatchListFilesParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile>;\n  listFiles(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile>;\n  listFiles(\n    vectorStoreId: string,\n    batchId: string,\n    query: FileBatchListFilesParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile> {\n    if (isRequestOptions(query)) {\n      return this.listFiles(vectorStoreId, batchId, {}, query);\n    }\n    return this._client.getAPIList(\n      `/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`,\n      VectorStoreFilesPage,\n      { query, ...options, headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers } },\n    );\n  }\n\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const headers: { [key: string]: string } = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\n    if (options?.pollIntervalMs) {\n      headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n    }\n\n    while (true) {\n      const { data: batch, response } = await this.retrieve(vectorStoreId, batchId, {\n        ...options,\n        headers,\n      }).withResponse();\n\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(\n    vectorStoreId: string,\n    { files, fileIds = [] }: { files: Uploadable[]; fileIds?: string[] },\n    options?: Core.RequestOptions & { pollIntervalMs?: number; maxConcurrency?: number },\n  ): Promise<VectorStoreFileBatch> {\n    if (files === null || files.length == 0) {\n      throw new Error('No files provided to process.');\n    }\n\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n    //We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds: string[] = [...fileIds];\n\n    //This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator: IterableIterator<Uploadable>) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n\n    //Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n\n    //Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds,\n    });\n  }\n}\n\n/**\n * A batch of files attached to a vector store.\n */\nexport interface VectorStoreFileBatch {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store files batch was\n   * created.\n   */\n  created_at: number;\n\n  file_counts: VectorStoreFileBatch.FileCounts;\n\n  /**\n   * The object type, which is always `vector_store.file_batch`.\n   */\n  object: 'vector_store.files_batch';\n\n  /**\n   * The status of the vector store files batch, which can be either `in_progress`,\n   * `completed`, `cancelled` or `failed`.\n   */\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\n\n  /**\n   * The ID of the\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\n   * attached to.\n   */\n  vector_store_id: string;\n}\n\nexport namespace VectorStoreFileBatch {\n  export interface FileCounts {\n    /**\n     * The number of files that where cancelled.\n     */\n    cancelled: number;\n\n    /**\n     * The number of files that have been processed.\n     */\n    completed: number;\n\n    /**\n     * The number of files that have failed to process.\n     */\n    failed: number;\n\n    /**\n     * The number of files that are currently being processed.\n     */\n    in_progress: number;\n\n    /**\n     * The total number of files.\n     */\n    total: number;\n  }\n}\n\nexport interface FileBatchCreateParams {\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the vector store should use. Useful for tools like `file_search` that can access\n   * files.\n   */\n  file_ids: Array<string>;\n}\n\nexport interface FileBatchListFilesParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.\n   */\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace FileBatches {\n  export import VectorStoreFileBatch = FileBatchesAPI.VectorStoreFileBatch;\n  export import FileBatchCreateParams = FileBatchesAPI.FileBatchCreateParams;\n  export import FileBatchListFilesParams = FileBatchesAPI.FileBatchListFilesParams;\n}\n\nexport { VectorStoreFilesPage };\n"]}},"error":null,"hash":"cf5fa045669be9f36944b33efd50e4ee","cacheData":{"env":{}}}